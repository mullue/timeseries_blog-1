{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Downloading geopandas-0.8.1-py2.py3-none-any.whl (962 kB)\n",
      "\u001b[K     |████████████████████████████████| 962 kB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from geopandas) (1.0.5)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Downloading pyproj-2.6.1.post1-cp36-cp36m-manylinux2010_x86_64.whl (10.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.9 MB 24.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fiona\n",
      "  Downloading Fiona-1.8.17-cp36-cp36m-manylinux1_x86_64.whl (14.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8 MB 42.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting shapely\n",
      "  Downloading Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 60.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.23.0->geopandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.23.0->geopandas) (1.18.1)\n",
      "Requirement already satisfied: six>=1.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from fiona->geopandas) (1.14.0)\n",
      "Collecting click-plugins>=1.0\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: click<8,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from fiona->geopandas) (7.1.1)\n",
      "Collecting munch\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting cligj>=0.5\n",
      "  Downloading cligj-0.5.0-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: attrs>=17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from fiona->geopandas) (19.3.0)\n",
      "Installing collected packages: pyproj, click-plugins, munch, cligj, fiona, shapely, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.5.0 fiona-1.8.17 geopandas-0.8.1 munch-2.5.0 pyproj-2.6.1.post1 shapely-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import boto3, time, json, warnings, os\n",
    "import urllib.request\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# the train test split date is used to split each time series into train and test sets\n",
    "train_test_split_date = date.today() - timedelta(days = 30)\n",
    "\n",
    "# the sampling frequency determines the number of hours per sample\n",
    "# and is used for aggregating and filling missing values\n",
    "frequency = '1'\n",
    "\n",
    "# prediction length is how many hours into future to predict values for\n",
    "prediction_length = 48\n",
    "\n",
    "# context length is how many prior time steps the predictor needs to make a prediction\n",
    "context_length = 3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def get_athena_s3_staging_dir():\n",
    "    session = boto3.Session()\n",
    "    account_id = session.client('sts').get_caller_identity().get('Account')\n",
    "    return f's3://{account_id}-openaq-lab/athena/results/'\n",
    "    \n",
    "# processing Athena\n",
    "def athena_query_table(query_file, wait=None):\n",
    "    results_uri = athena_execute(query_file, 'csv', wait)\n",
    "    return results_uri\n",
    "\n",
    "def athena_execute(query_file, ext, wait):\n",
    "    with open(query_file) as f:\n",
    "        query_str = f.read()  \n",
    "        \n",
    "    athena = boto3.client('athena')\n",
    "    s3_dest = get_athena_s3_staging_dir()\n",
    "    query_id = athena.start_query_execution(\n",
    "        QueryString= query_str, \n",
    "         ResultConfiguration={'OutputLocation': s3_dest}\n",
    "    )['QueryExecutionId']\n",
    "        \n",
    "    results_uri = f'{s3_dest}{query_id}.{ext}'\n",
    "        \n",
    "    start = time.time()\n",
    "    while wait == None or wait == 0 or time.time() - start < wait:\n",
    "        result = athena.get_query_execution(QueryExecutionId=query_id)\n",
    "        status = result['QueryExecution']['Status']['State']\n",
    "        if wait == 0 or status == 'SUCCEEDED':\n",
    "            break\n",
    "        elif status in ['QUEUED','RUNNING']:\n",
    "            continue\n",
    "        else:\n",
    "            raise Exception(f'query {query_id} failed with status {status}')\n",
    "\n",
    "            time.sleep(3) \n",
    "\n",
    "    return results_uri       \n",
    "\n",
    "def get_sydney_openaq_data(sql_query_file_path = \"/opt/ml/processing/sql/sydney.dml\"):\n",
    "    query_results_uri = athena_query_table(sql_query_file_path)\n",
    "    print (f'reading {query_results_uri}')\n",
    "#     s3 = boto3.resource('s3')\n",
    "#     s3.meta.client.download_file('mybucket', 'hello.txt', '/tmp/hello.txt')\n",
    "    raw = pd.read_csv(query_results_uri, parse_dates=['timestamp'])\n",
    "    return raw\n",
    "\n",
    "raw = get_sydney_openaq_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received arguments Namespace(region='us-east-1', split_days=30)\n",
      "Saving all features to /opt/ml/processing/output/all_features.csv\n",
      "Saving train features to /opt/ml/processing/output/train.csv\n",
      "Saving test features to /opt/ml/processing/output/test.csv\n"
     ]
    }
   ],
   "source": [
    "def featurize(raw):\n",
    "\n",
    "    def fill_missing_hours(df):\n",
    "        df = df.reset_index(level=categorical_levels, drop=True)                                    \n",
    "        index = pd.date_range(df.index.min(), df.index.max(), freq='1H')\n",
    "        return df.reindex(pd.Index(index, name='timestamp'))\n",
    "\n",
    "    # Sort and index by location and time\n",
    "    categorical_levels = ['country', 'city', 'location', 'parameter']\n",
    "    index_levels = categorical_levels + ['timestamp']\n",
    "    indexed = raw.sort_values(index_levels, ascending=True)\n",
    "    indexed = indexed.set_index(index_levels)\n",
    "    # indexed.head()    \n",
    "    \n",
    "    # Downsample to hourly samples by maximum value\n",
    "    downsampled = indexed.groupby(categorical_levels + [pd.Grouper(level='timestamp', freq='1H')]).max()\n",
    "\n",
    "    # Back fill missing values\n",
    "    filled = downsampled.groupby(level=categorical_levels).apply(fill_missing_hours)\n",
    "    filled[filled['value'].isnull()].groupby('location').count().describe()\n",
    "    \n",
    "    filled['value'] = filled['value'].interpolate().round(2)\n",
    "    filled['point_latitude'] = filled['point_latitude'].fillna(method='pad')\n",
    "    filled['point_longitude'] = filled['point_longitude'].fillna(method='pad')\n",
    "\n",
    "    # Create Features\n",
    "    aggregated = filled.reset_index(level=4)\\\n",
    "        .groupby(level=categorical_levels)\\\n",
    "        .agg(dict(timestamp='first', value=list, point_latitude='first', point_longitude='first'))\\\n",
    "        .rename(columns=dict(timestamp='start', value='target'))    \n",
    "    aggregated['id'] = np.arange(len(aggregated))\n",
    "    aggregated.reset_index(inplace=True)\n",
    "    aggregated.set_index(['id']+categorical_levels, inplace=True)\n",
    "    \n",
    "    metadata = gpd.GeoDataFrame(\n",
    "        aggregated.drop(columns=['target','start']), \n",
    "        geometry=gpd.points_from_xy(aggregated.point_longitude, aggregated.point_latitude), \n",
    "        crs={\"init\":\"EPSG:4326\"}\n",
    "    )\n",
    "    metadata.drop(columns=['point_longitude', 'point_latitude'], inplace=True)\n",
    "    # set geometry index\n",
    "    metadata.set_geometry('geometry')\n",
    "\n",
    "    # Add Categorical features\n",
    "    level_ids = [level+'_id' for level in categorical_levels]\n",
    "    for l in level_ids:\n",
    "        aggregated[l], index = pd.factorize(aggregated.index.get_level_values(l[:-3]))\n",
    "\n",
    "    aggregated['cat'] = aggregated.apply(lambda columns: [columns[l] for l in level_ids], axis=1)\n",
    "    features = aggregated.drop(columns=level_ids+ ['point_longitude', 'point_latitude'])\n",
    "    features.reset_index(level=categorical_levels, inplace=True, drop=True)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def filter_dates(df, min_time, max_time, frequency):\n",
    "    min_time = None if min_time is None else pd.to_datetime(min_time)\n",
    "    max_time = None if max_time is None else pd.to_datetime(max_time)\n",
    "    interval = pd.Timedelta(frequency)\n",
    "    \n",
    "    def _filter_dates(r): \n",
    "        if min_time is not None and r['start'] < min_time:\n",
    "            start_idx = int((min_time - r['start']) / interval)\n",
    "            r['target'] = r['target'][start_idx:]\n",
    "            r['start'] = min_time\n",
    "        \n",
    "        end_time = r['start'] + len(r['target']) * interval\n",
    "        if max_time is not None and end_time > max_time:\n",
    "            end_idx = int((end_time - max_time) / interval)\n",
    "            r['target'] = r['target'][:-end_idx]\n",
    "            \n",
    "        return r\n",
    "    \n",
    "    filtered = df.apply(_filter_dates, axis=1) \n",
    "    filtered = filtered[filtered['target'].str.len() > 0]\n",
    "    return filtered\n",
    "\n",
    "def split_train_test_data(features, days = 30):\n",
    "    train_test_split_date = date.today() - timedelta(days = days)\n",
    "    train = filter_dates(features, None, train_test_split_date, '1H')\n",
    "    test = filter_dates(features, train_test_split_date, None, '1H')\n",
    "    return train, test\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--split-days\", type=int, default=30)\n",
    "parser.add_argument(\"--region\", type=str, default='us-east-1')\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "print(\"Received arguments {}\".format(args))\n",
    "split_days = args.split_days\n",
    "region = args.region\n",
    "\n",
    "# definte environment variable\n",
    "os.environ['AWS_DEFAULT_REGION'] = region\n",
    "\n",
    "features = featurize(raw)\n",
    "train, test = split_train_test_data(features)\n",
    "\n",
    "all_features_output_path = os.path.join(\n",
    "    \"/opt/ml/processing/output\", \"all_features.csv\"\n",
    ")\n",
    "print(\"Saving all features to {}\".format(all_features_output_path))\n",
    "features.to_json(all_features_output_path, orient='records', lines = True)\n",
    "\n",
    "train_features_output_path = os.path.join(\n",
    "    \"/opt/ml/processing/output\", \"train.csv\"\n",
    ")\n",
    "print(\"Saving train features to {}\".format(train_features_output_path))\n",
    "train.to_json(train_features_output_path, orient='records', lines = True)\n",
    "\n",
    "test_features_output_path = os.path.join(\n",
    "    \"/opt/ml/processing/output\", \"test.csv\"\n",
    ")\n",
    "print(\"Saving test features to {}\".format(test_features_output_path))\n",
    "test.to_json(test_features_output_path, orient='records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-09-08 21:00:00</td>\n",
       "      <td>[5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....</td>\n",
       "      <td>[0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>[5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...</td>\n",
       "      <td>[0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...</td>\n",
       "      <td>[0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start                                             target  \\\n",
       "id                                                                          \n",
       "0  2016-04-09 20:00:00  [12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....   \n",
       "1  2019-09-08 21:00:00  [5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...   \n",
       "2  2016-04-09 20:00:00  [11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....   \n",
       "3  2017-08-31 00:00:00  [5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...   \n",
       "4  2016-04-09 20:00:00  [20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...   \n",
       "\n",
       "             cat  \n",
       "id                \n",
       "0   [0, 0, 0, 0]  \n",
       "1   [0, 0, 1, 0]  \n",
       "2   [0, 0, 2, 0]  \n",
       "3   [0, 0, 3, 0]  \n",
       "4   [0, 0, 4, 0]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
