{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline for \"Forecasting Air Quality with Amazon SageMaker DeepAR\n",
    "\n",
    "In this example, we are going to build a ML Pipeline to automate air quality forecasting application with [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline\n",
    "\n",
    "### Outcome\n",
    "* Create the flow for ML process for air quality forcasting build/train/deploy\n",
    "* Create simple retrain flow\n",
    "\n",
    "### Design\n",
    "* Use Step Functions Data Science SDK to orchestrate the ML flow\n",
    "* Use SageMaker Processing to do data preprocessing, especially,\n",
    " * A common Docker image will be build for data retrieving (interact with Amazon Athena) and data/feature engineering\n",
    "* Use SageMaker Processing to do Model Evaluation\n",
    "* A scheduled job mechanism will be used to do model retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker==1.71.0\"\n",
    "!{sys.executable} -m pip install -qU \"stepfunctions==1.1.0\"\n",
    "!{sys.executable} -m pip show sagemaker stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import boto3\n",
    "import stepfunctions\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.steps import (\n",
    "    Chain,\n",
    "    ChoiceRule,\n",
    "    ModelStep,\n",
    "    ProcessingStep,\n",
    "    TrainingStep,\n",
    "    TransformStep\n",
    ")\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Docker Image for SageMaker Processing\n",
    "\n",
    "Define your own processing container and install related dependencies.\n",
    "\n",
    "Below, you talk through how to create a processing container, and how to use a `ScriptProcessor` to run your own code within a container. Create a container support data preprocessing, feature engineering and model evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subfolder for docker \n",
    "!mkdir -p docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the Dockerfile to create processing container. Install PyAthena, pandas and GeoPandas into it. You can install your own dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "    \n",
    "COPY ./sql /opt/ml/processing/sql\n",
    "    \n",
    "RUN pip install pandas numpy geopandas scikit-learn fsspec s3fs boto3\n",
    "\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code buils the container using the docker command, creates an Amazon Elastic Container Registry (Amazon ECR) repository, and pushes the image to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'aq-forecasting-processing-container'\n",
    "tag = ':latest'\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo consider using CFN template to create ECR repo and only manage the docker image build and push.\n",
    "!docker build -t $ecr_repository docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell writes a file `preprocessing.py`, which contains the pre-processing script. You can update the script, and rerun the cell to overwrite `preprocessing.py`. You run this as a processing job in the next cell. In this script, the actions will be done:\n",
    "\n",
    "* Create Athena table with external source - OpenAQ\n",
    "* Query OpenAQ data \n",
    "* Feature engineering on the dataset\n",
    "* Split and store the data on S3 buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the pre processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_SCRIPT_LOCATION = \"preprocessing.py\"\n",
    "\n",
    "input_code = sagemaker_session.upload_data(\n",
    "    PREPROCESSING_SCRIPT_LOCATION,\n",
    "    bucket = sagemaker_session.default_bucket(),\n",
    "    key_prefix = \"processing/code\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 locations of preprocessing output and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_base_uri = f\"s3://{sagemaker_session.default_bucket()}\"\n",
    "output_data = f\"{s3_bucket_base_uri}/preprocessing_data/output\"\n",
    "#preprocessed_training_data = f\"{output_data}/train_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScriptProcessor` class lets you run a command inside the container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "preprocessing_processor = ScriptProcessor(\n",
    "    command = ['python3'],\n",
    "    image_uri = processing_repository_uri,\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    "    max_runtime_in_seconds = 1200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the ProcessingStep\n",
    "We will now create the [ProcessingStep](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/sagemaker.html#stepfunctions.steps.sagemaker.ProcessingStep) that will launch a SageMaker Processing Job.\n",
    "\n",
    "This step will use ScriptProcessor as defined in previous steps along with the inputs and outputs objects that are defined in the below steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    ProcessingInput(\n",
    "        source = input_code,\n",
    "        destination = \"/opt/ml/processing/input/code\",\n",
    "        input_name = \"code\"\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    ProcessingOutput(\n",
    "        source = \"/opt/ml/processing/output\",\n",
    "        destination = f\"{output_data}\",\n",
    "        output_name = \"output_data\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(\n",
    "    schema = {\n",
    "        \"PreprocessingJobName\": str,\n",
    "        \"TrainingJobName\": str,\n",
    "        \"TuningJobName\": str,\n",
    "        \"EvaluationProcessingJobName\": str\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ProcessingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = ProcessingStep(\n",
    "    \"Air Quality Forecasting pre-processing step\",\n",
    "    processor = preprocessing_processor,\n",
    "    job_name = execution_input[\"PreprocessingJobName\"],\n",
    "    inputs = inputs,\n",
    "    outputs = outputs,\n",
    "    container_arguments = [\"--split-days\", \"30\"],\n",
    "    container_entrypoint = [\"python3\", \"/opt/ml/processing/input/code/preprocessing.py\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Using the pre-processed data\n",
    "\n",
    "We create a DeepAR instance, which we will use to run a training job. This will be used to create a TrainingStep for the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "Run model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `Fail` state to mark the workflow failed in case any of the steps fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_state_sagemaker_processing_failure = stepfunctions.steps.states.Fail(\n",
    "    \"ML Workflow failed\", cause = \"SageMakerProcessingJobFailed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the Error handling in the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_state_processing = stepfunctions.steps.states.Catch(\n",
    "    error_equals = [\"States.TaskFailed\"],\n",
    "    next_step = failed_state_sagemaker_processing_failure   \n",
    ")\n",
    "processing_step.add_catch(catch_state_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_execution_role = \"arn:aws:iam::593380422482:role/StepFunctionsWorkflowExecutionRole\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create StepFunctions Workflow execution Input schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_name = f\"aqf-preprocessing-{uuid.uuid1().hex}\"\n",
    "training_job_name = f\"aqf-training-{uuid.uuid1().hex}\"\n",
    "tuning_job_name = f\"aqf-tuning-{uuid.uuid1().hex}\"\n",
    "evaluation_job_name = f\"aqf-evaluation-{uuid.uuid1().hex}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preprocessing_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and execute the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_graph = Chain([processing_step])\n",
    "workflow = Workflow(\n",
    "    name = \"AirQualityForecastingWorkflow\",\n",
    "    definition = workflow_graph,\n",
    "    role = workflow_execution_role\n",
    ")\n",
    "workflow.create()\n",
    "\n",
    "# execute workflow\n",
    "execution = workflow.execute(\n",
    "    inputs = {\n",
    "        \"PreprocessingJobName\": preprocessing_job_name,\n",
    "        \"TrainingJobName\": training_job_name,\n",
    "        \"TuningJobName\": tuning_job_name,\n",
    "        \"EvaluationProcessingJobName\": evaluation_job_name\n",
    "    }\n",
    ")\n",
    "execution_output = execution.get_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
