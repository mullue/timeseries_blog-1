{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline for \"Forecasting Air Quality with Amazon SageMaker DeepAR\n",
    "\n",
    "In this example, we are going to build a ML Pipeline to automate air quality forecasting application with [AWS Step Functions Data Science SDK](https://aws-step-functions-data-science-sdk.readthedocs.io). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline\n",
    "\n",
    "### Outcome\n",
    "* Create the flow for ML process for air quality forcasting build/train/deploy\n",
    "* Create simple retrain flow\n",
    "\n",
    "### Design\n",
    "* Use Step Functions Data Science SDK to orchestrate the ML flow\n",
    "* Use SageMaker Processing to do data preprocessing, especially,\n",
    " * A common Docker image will be build for data retrieving (interact with Amazon Athena) and data/feature engineering\n",
    "* Use SageMaker Processing to do Model Evaluation\n",
    "* A scheduled job mechanism will be used to do model retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Docker Image for SageMaker Processing\n",
    "\n",
    "Define your own processing container and install related dependencies.\n",
    "\n",
    "Below, you talk through how to create a processing container, and how to use a `ScriptProcessor` to run your own code within a container. Create a container support data preprocessing, feature engineering and model evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subfolder for docker \n",
    "!mkdir -p docker\n",
    "!cp -r ../sql ./docker/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the Dockerfile to create processing container. Install PyAthena, pandas and GeoPandas into it. You can install your own dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "    \n",
    "COPY ./sql /opt/ml/processing/\n",
    "    \n",
    "RUN pip install PyAthena[Pandas] geopandas scikit-learn\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code buils the container using the docker command, creates an Amazon Elastic Container Registry (Amazon ECR) repository, and pushes the image to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "ecr_repository = 'aq-forecasting-processing-container'\n",
    "tag = ':latest'\n",
    "\n",
    "uri_suffix = 'amazonaws.com'\n",
    "if region in ['cn-north-1', 'cn-northwest-1']:\n",
    "    uri_suffix = 'amazonaws.com.cn'\n",
    "processing_repository_uri = f'{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'593380422482.dkr.ecr.us-east-1.amazonaws.com/aq-forecasting-processing-container:latest'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @todo consider using CFN template to create ECR repo and only manage the docker image build and push.\n",
    "!docker build -t $ecr_repository docker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell writes a file `preprocessing.py`, which contains the pre-processing script. You can update the script, and rerun the cell to overwrite `preprocessing.py`. You run this as a processing job in the next cell. In this script, the actions will be done:\n",
    "\n",
    "* Create Athena table with external source - OpenAQ\n",
    "* Query OpenAQ data \n",
    "* Feature engineering on the dataset\n",
    "* Split and store the data on S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading s3://593380422482-openaq-lab/athena/results/cdd9bf67-f051-4f6b-8efe-509915d9cc84.csv\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import boto3, time, s3fs, json, warnings, os\n",
    "import urllib.request\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# the train test split date is used to split each time series into train and test sets\n",
    "train_test_split_date = date.today() - timedelta(days = 30)\n",
    "\n",
    "# the sampling frequency determines the number of hours per sample\n",
    "# and is used for aggregating and filling missing values\n",
    "frequency = '1'\n",
    "\n",
    "# prediction length is how many hours into future to predict values for\n",
    "prediction_length = 48\n",
    "\n",
    "# context length is how many prior time steps the predictor needs to make a prediction\n",
    "context_length = 3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = session.region_name\n",
    "account = session.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = f\"{account_id}-openaq-lab\"\n",
    "athena_s3_staging_dir = f's3://{bucket_name}/athena/results/'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# @todo to evaluate whether we should store existing model.tar.gz onto s3 bucket.\n",
    "\n",
    "# processing Athena\n",
    "def athena_query_table(query_file, wait=None):\n",
    "    results_uri = athena_execute(query_file, 'csv', wait)\n",
    "    return results_uri\n",
    "\n",
    "def athena_execute(query_file, ext, wait):\n",
    "    with open(query_file) as f:\n",
    "        query_str = f.read()  \n",
    "        \n",
    "    athena = boto3.client('athena')\n",
    "    s3_dest = athena_s3_staging_dir\n",
    "    query_id = athena.start_query_execution(\n",
    "        QueryString= query_str, \n",
    "         ResultConfiguration={'OutputLocation': athena_s3_staging_dir}\n",
    "    )['QueryExecutionId']\n",
    "        \n",
    "    results_uri = f'{athena_s3_staging_dir}{query_id}.{ext}'\n",
    "        \n",
    "    start = time.time()\n",
    "    while wait == None or wait == 0 or time.time() - start < wait:\n",
    "        result = athena.get_query_execution(QueryExecutionId=query_id)\n",
    "        status = result['QueryExecution']['Status']['State']\n",
    "        if wait == 0 or status == 'SUCCEEDED':\n",
    "            break\n",
    "        elif status in ['QUEUED','RUNNING']:\n",
    "            continue\n",
    "        else:\n",
    "            raise Exception(f'query {query_id} failed with status {status}')\n",
    "\n",
    "            time.sleep(3) \n",
    "\n",
    "    return results_uri       \n",
    "\n",
    "def get_sydney_openaq_data(sql_query_file_path = \"/opt/ml/processing/sql/sydney.dml\"):\n",
    "    query_results_uri = athena_query_table(sql_query_file_path)\n",
    "    print (f'reading {query_results_uri}')\n",
    "    raw = pd.read_csv(query_results_uri, parse_dates=['timestamp'])\n",
    "    return raw\n",
    "\n",
    "raw = get_sydney_openaq_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(raw):\n",
    "\n",
    "    def fill_missing_hours(df):\n",
    "        df = df.reset_index(level=categorical_levels, drop=True)                                    \n",
    "        index = pd.date_range(df.index.min(), df.index.max(), freq='1H')\n",
    "        return df.reindex(pd.Index(index, name='timestamp'))\n",
    "\n",
    "    # Sort and index by location and time\n",
    "    categorical_levels = ['country', 'city', 'location', 'parameter']\n",
    "    index_levels = categorical_levels + ['timestamp']\n",
    "    indexed = raw.sort_values(index_levels, ascending=True)\n",
    "    indexed = indexed.set_index(index_levels)\n",
    "    # indexed.head()    \n",
    "    \n",
    "    # Downsample to hourly samples by maximum value\n",
    "    downsampled = indexed.groupby(categorical_levels + [pd.Grouper(level='timestamp', freq='1H')]).max()\n",
    "\n",
    "    # Back fill missing values\n",
    "    filled = downsampled.groupby(level=categorical_levels).apply(fill_missing_hours)\n",
    "    filled[filled['value'].isnull()].groupby('location').count().describe()\n",
    "    \n",
    "    filled['value'] = filled['value'].interpolate().round(2)\n",
    "    filled['point_latitude'] = filled['point_latitude'].fillna(method='pad')\n",
    "    filled['point_longitude'] = filled['point_longitude'].fillna(method='pad')\n",
    "\n",
    "    # Create Features\n",
    "    aggregated = filled.reset_index(level=4)\\\n",
    "        .groupby(level=categorical_levels)\\\n",
    "        .agg(dict(timestamp='first', value=list, point_latitude='first', point_longitude='first'))\\\n",
    "        .rename(columns=dict(timestamp='start', value='target'))    \n",
    "    aggregated['id'] = np.arange(len(aggregated))\n",
    "    aggregated.reset_index(inplace=True)\n",
    "    aggregated.set_index(['id']+categorical_levels, inplace=True)\n",
    "    \n",
    "    metadata = gpd.GeoDataFrame(\n",
    "        aggregated.drop(columns=['target','start']), \n",
    "        geometry=gpd.points_from_xy(aggregated.point_longitude, aggregated.point_latitude), \n",
    "        crs={\"init\":\"EPSG:4326\"}\n",
    "    )\n",
    "    metadata.drop(columns=['point_longitude', 'point_latitude'], inplace=True)\n",
    "    # set geometry index\n",
    "    metadata.set_geometry('geometry')\n",
    "\n",
    "    # Add Categorical features\n",
    "    level_ids = [level+'_id' for level in categorical_levels]\n",
    "    for l in level_ids:\n",
    "        aggregated[l], index = pd.factorize(aggregated.index.get_level_values(l[:-3]))\n",
    "\n",
    "    aggregated['cat'] = aggregated.apply(lambda columns: [columns[l] for l in level_ids], axis=1)\n",
    "    features = aggregated.drop(columns=level_ids+ ['point_longitude', 'point_latitude'])\n",
    "    features.reset_index(level=categorical_levels, inplace=True, drop=True)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def filter_dates(df, min_time, max_time, frequency):\n",
    "    min_time = None if min_time is None else pd.to_datetime(min_time)\n",
    "    max_time = None if max_time is None else pd.to_datetime(max_time)\n",
    "    interval = pd.Timedelta(frequency)\n",
    "    \n",
    "    def _filter_dates(r): \n",
    "        if min_time is not None and r['start'] < min_time:\n",
    "            start_idx = int((min_time - r['start']) / interval)\n",
    "            r['target'] = r['target'][start_idx:]\n",
    "            r['start'] = min_time\n",
    "        \n",
    "        end_time = r['start'] + len(r['target']) * interval\n",
    "        if max_time is not None and end_time > max_time:\n",
    "            end_idx = int((end_time - max_time) / interval)\n",
    "            r['target'] = r['target'][:-end_idx]\n",
    "            \n",
    "        return r\n",
    "    \n",
    "    filtered = df.apply(_filter_dates, axis=1) \n",
    "    filtered = filtered[filtered['target'].str.len() > 0]\n",
    "    return filtered\n",
    "\n",
    "def split_train_test_data(features, days = 30):\n",
    "    train_test_split_date = date.today() - timedelta(days = days)\n",
    "    train = filter_dates(features, None, train_test_split_date, '1H')\n",
    "    test = filter_dates(features, train_test_split_date, None, '1H')\n",
    "    return train, test\n",
    "\n",
    "features = featurize(raw)\n",
    "train, test = split_train_test_data(features)\n",
    "\n",
    "# upload dataset to S3.\n",
    "local_data_path = 'data'\n",
    "os.makedirs(local_data_path, exist_ok = True)\n",
    "train.to_json(f'{local_data_path}/train.json', orient='records', lines = True)\n",
    "train_data_uri = sagemaker_session.upload_data(path=f'{local_data_path}/train.json', key_prefix = 'preprocessing_data')\n",
    "\n",
    "test.to_json(f'{local_data_path}/test.json', orient='records', lines = True) \n",
    "test_data_uri =  sagemaker_session.upload_data(path=f'{local_data_path}/test.json', key_prefix = 'preprocessing_data')\n",
    "\n",
    "features.to_json(f'{local_data_path}/all_data.json', orient='records', lines = True)\n",
    "all_data_uri = sagemaker_session.upload_data(path=f'{local_data_path}/all_data.json', key_prefix = 'preprocessing_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-09-08 21:00:00</td>\n",
       "      <td>[5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....</td>\n",
       "      <td>[0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>[5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...</td>\n",
       "      <td>[0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...</td>\n",
       "      <td>[0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start                                             target  \\\n",
       "id                                                                          \n",
       "0  2016-04-09 20:00:00  [12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....   \n",
       "1  2019-09-08 21:00:00  [5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...   \n",
       "2  2016-04-09 20:00:00  [11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....   \n",
       "3  2017-08-31 00:00:00  [5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...   \n",
       "4  2016-04-09 20:00:00  [20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...   \n",
       "\n",
       "             cat  \n",
       "id                \n",
       "0   [0, 0, 0, 0]  \n",
       "1   [0, 0, 1, 0]  \n",
       "2   [0, 0, 2, 0]  \n",
       "3   [0, 0, 3, 0]  \n",
       "4   [0, 0, 4, 0]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start     18\n",
       "target    18\n",
       "cat       18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-09-08 21:00:00</td>\n",
       "      <td>[5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....</td>\n",
       "      <td>[0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-31 00:00:00</td>\n",
       "      <td>[5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...</td>\n",
       "      <td>[0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-04-09 20:00:00</td>\n",
       "      <td>[20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...</td>\n",
       "      <td>[0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start                                             target  \\\n",
       "id                                                                          \n",
       "0  2016-04-09 20:00:00  [12.2, 12.3, 12.4, 12.5, 12.8, 13.3, 13.3, 13....   \n",
       "1  2019-09-08 21:00:00  [5.2, 5.4, 5.6, 5.8, 6.0, 6.1, 6.4, 6.7, 7.2, ...   \n",
       "2  2016-04-09 20:00:00  [11.2, 11.3, 11.5, 11.6, 11.4, 11.3, 11.2, 11....   \n",
       "3  2017-08-31 00:00:00  [5.5, 5.3, 5.3, 5.3, 5.2, 4.9, 4.8, 4.8, 4.8, ...   \n",
       "4  2016-04-09 20:00:00  [20.2, 20.55, 20.9, 20.2, 19.1, 18.2, 17.8, 18...   \n",
       "\n",
       "             cat  \n",
       "id                \n",
       "0   [0, 0, 0, 0]  \n",
       "1   [0, 0, 1, 0]  \n",
       "2   [0, 0, 2, 0]  \n",
       "3   [0, 0, 3, 0]  \n",
       "4   [0, 0, 4, 0]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>target</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>[3.4, 0.5, 14.8, 0.8, 1.0, 3.0, 3.9, 6.2, 6.2,...</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>[5.9, 9.1, 12.9, 15.3, 10.6, 5.5, 7.3, 17.6, 1...</td>\n",
       "      <td>[0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>[5.1, 4.9, 8.2, 3.1, 3.2, 3.7, 3.8, 5.0, 6.1, ...</td>\n",
       "      <td>[0, 0, 2, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>[0.0, 1.5, 10.3, 0.0, 0.0, 0.9, 1.0, 3.4, 3.7,...</td>\n",
       "      <td>[0, 0, 3, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-15</td>\n",
       "      <td>[3.7, 2.0, 5.4, 1.2, 2.6, 3.6, 3.5, 5.4, 4.2, ...</td>\n",
       "      <td>[0, 0, 4, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start                                             target           cat\n",
       "id                                                                            \n",
       "0  2020-08-15  [3.4, 0.5, 14.8, 0.8, 1.0, 3.0, 3.9, 6.2, 6.2,...  [0, 0, 0, 0]\n",
       "1  2020-08-15  [5.9, 9.1, 12.9, 15.3, 10.6, 5.5, 7.3, 17.6, 1...  [0, 0, 1, 0]\n",
       "2  2020-08-15  [5.1, 4.9, 8.2, 3.1, 3.2, 3.7, 3.8, 5.0, 6.1, ...  [0, 0, 2, 0]\n",
       "3  2020-08-15  [0.0, 1.5, 10.3, 0.0, 0.0, 0.9, 1.0, 3.4, 3.7,...  [0, 0, 3, 0]\n",
       "4  2020-08-15  [3.7, 2.0, 5.4, 1.2, 2.6, 3.6, 3.5, 5.4, 4.2, ...  [0, 0, 4, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import boto3, time, s3fs, json, warnings, os\n",
    "import urllib.request\n",
    "from datetime import date, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# the train test split date is used to split each time series into train and test sets\n",
    "train_test_split_date = date.today() - timedelta(days = 30)\n",
    "\n",
    "# the sampling frequency determines the number of hours per sample\n",
    "# and is used for aggregating and filling missing values\n",
    "frequency = '1'\n",
    "\n",
    "# prediction length is how many hours into future to predict values for\n",
    "prediction_length = 48\n",
    "\n",
    "# context length is how many prior time steps the predictor needs to make a prediction\n",
    "context_length = 3\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = session.region_name\n",
    "account = session.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = f\"{account_id}-openaq-lab\"\n",
    "athena_s3_staging_dir = f's3://{bucket_name}/athena/results/'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# @todo to evaluate whether we should store existing model.tar.gz onto s3 bucket.\n",
    "\n",
    "# processing Athena\n",
    "def athena_query_table(query_file, wait=None):\n",
    "    results_uri = athena_execute(query_file, 'csv', wait)\n",
    "    return results_uri\n",
    "\n",
    "def athena_execute(query_file, ext, wait):\n",
    "    with open(query_file) as f:\n",
    "        query_str = f.read()  \n",
    "        \n",
    "    athena = boto3.client('athena')\n",
    "    s3_dest = athena_s3_staging_dir\n",
    "    query_id = athena.start_query_execution(\n",
    "        QueryString= query_str, \n",
    "         ResultConfiguration={'OutputLocation': athena_s3_staging_dir}\n",
    "    )['QueryExecutionId']\n",
    "        \n",
    "    results_uri = f'{athena_s3_staging_dir}{query_id}.{ext}'\n",
    "        \n",
    "    start = time.time()\n",
    "    while wait == None or wait == 0 or time.time() - start < wait:\n",
    "        result = athena.get_query_execution(QueryExecutionId=query_id)\n",
    "        status = result['QueryExecution']['Status']['State']\n",
    "        if wait == 0 or status == 'SUCCEEDED':\n",
    "            break\n",
    "        elif status in ['QUEUED','RUNNING']:\n",
    "            continue\n",
    "        else:\n",
    "            raise Exception(f'query {query_id} failed with status {status}')\n",
    "\n",
    "            time.sleep(3) \n",
    "\n",
    "    return results_uri       \n",
    "\n",
    "def get_sydney_openaq_data(sql_query_file_path = \"/opt/ml/processing/sql/sydney.dml\"):\n",
    "    query_results_uri = athena_query_table(sql_query_file_path)\n",
    "    print (f'reading {query_results_uri}')\n",
    "    raw = pd.read_csv(query_results_uri, parse_dates=['timestamp'])\n",
    "    return raw\n",
    "\n",
    "def featurize(raw):\n",
    "\n",
    "    def fill_missing_hours(df):\n",
    "        df = df.reset_index(level=categorical_levels, drop=True)                                    \n",
    "        index = pd.date_range(df.index.min(), df.index.max(), freq='1H')\n",
    "        return df.reindex(pd.Index(index, name='timestamp'))\n",
    "\n",
    "    # Sort and index by location and time\n",
    "    categorical_levels = ['country', 'city', 'location', 'parameter']\n",
    "    index_levels = categorical_levels + ['timestamp']\n",
    "    indexed = raw.sort_values(index_levels, ascending=True)\n",
    "    indexed = indexed.set_index(index_levels)\n",
    "    # indexed.head()    \n",
    "    \n",
    "    # Downsample to hourly samples by maximum value\n",
    "    downsampled = indexed.groupby(categorical_levels + [pd.Grouper(level='timestamp', freq='1H')]).max()\n",
    "\n",
    "    # Back fill missing values\n",
    "    filled = downsampled.groupby(level=categorical_levels).apply(fill_missing_hours)\n",
    "    filled[filled['value'].isnull()].groupby('location').count().describe()\n",
    "    \n",
    "    filled['value'] = filled['value'].interpolate().round(2)\n",
    "    filled['point_latitude'] = filled['point_latitude'].fillna(method='pad')\n",
    "    filled['point_longitude'] = filled['point_longitude'].fillna(method='pad')\n",
    "\n",
    "    # Create Features\n",
    "    aggregated = filled.reset_index(level=4)\\\n",
    "        .groupby(level=categorical_levels)\\\n",
    "        .agg(dict(timestamp='first', value=list, point_latitude='first', point_longitude='first'))\\\n",
    "        .rename(columns=dict(timestamp='start', value='target'))    \n",
    "    aggregated['id'] = np.arange(len(aggregated))\n",
    "    aggregated.reset_index(inplace=True)\n",
    "    aggregated.set_index(['id']+categorical_levels, inplace=True)\n",
    "    \n",
    "    metadata = gpd.GeoDataFrame(\n",
    "        aggregated.drop(columns=['target','start']), \n",
    "        geometry=gpd.points_from_xy(aggregated.point_longitude, aggregated.point_latitude), \n",
    "        crs={\"init\":\"EPSG:4326\"}\n",
    "    )\n",
    "    metadata.drop(columns=['point_longitude', 'point_latitude'], inplace=True)\n",
    "    # set geometry index\n",
    "    metadata.set_geometry('geometry')\n",
    "\n",
    "    # Add Categorical features\n",
    "    level_ids = [level+'_id' for level in categorical_levels]\n",
    "    for l in level_ids:\n",
    "        aggregated[l], index = pd.factorize(aggregated.index.get_level_values(l[:-3]))\n",
    "\n",
    "    aggregated['cat'] = aggregated.apply(lambda columns: [columns[l] for l in level_ids], axis=1)\n",
    "    features = aggregated.drop(columns=level_ids+ ['point_longitude', 'point_latitude'])\n",
    "    features.reset_index(level=categorical_levels, inplace=True, drop=True)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def filter_dates(df, min_time, max_time, frequency):\n",
    "    min_time = None if min_time is None else pd.to_datetime(min_time)\n",
    "    max_time = None if max_time is None else pd.to_datetime(max_time)\n",
    "    interval = pd.Timedelta(frequency)\n",
    "    \n",
    "    def _filter_dates(r): \n",
    "        if min_time is not None and r['start'] < min_time:\n",
    "            start_idx = int((min_time - r['start']) / interval)\n",
    "            r['target'] = r['target'][start_idx:]\n",
    "            r['start'] = min_time\n",
    "        \n",
    "        end_time = r['start'] + len(r['target']) * interval\n",
    "        if max_time is not None and end_time > max_time:\n",
    "            end_idx = int((end_time - max_time) / interval)\n",
    "            r['target'] = r['target'][:-end_idx]\n",
    "            \n",
    "        return r\n",
    "    \n",
    "    filtered = df.apply(_filter_dates, axis=1) \n",
    "    filtered = filtered[filtered['target'].str.len() > 0]\n",
    "    return filtered\n",
    "\n",
    "def split_train_test_data(features, days = 30):\n",
    "    train_test_split_date = date.today() - timedelta(days = days)\n",
    "    train = filter_dates(features, None, train_test_split_date, '1H')\n",
    "    test = filter_dates(features, train_test_split_date, None, '1H')\n",
    "    return train, test\n",
    "\n",
    "raw = get_sydney_openaq_data()\n",
    "features = featurize(raw)\n",
    "train, test = split_train_test_data(features)\n",
    "\n",
    "# upload dataset to S3.\n",
    "local_data_path = 'data'\n",
    "os.makedirs(local_data_path, exist_ok = True)\n",
    "train.to_json(f'{local_data_path}/train.json', orient='records', lines = True)\n",
    "train_data_uri = sagemaker_session.upload_data(path=f'{local_data_path}/train.json', key_prefix = 'preprocessing_data')\n",
    "\n",
    "test.to_json(f'{local_data_path}/test.json', orient='records', lines = True) \n",
    "test_data_uri =  sagemaker_session.upload_data(path=f'{local_data_path}/test.json', key_prefix = 'preprocessing_data')\n",
    "\n",
    "features.to_json(f'{local_data_path}/all_data.json', orient='records', lines = True)\n",
    "all_data_uri = sagemaker_session.upload_data(path=f'{local_data_path}/all_data.json', key_prefix = 'preprocessing_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the pre processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSING_SCRIPT_LOCATION = \"preprocessing.py\"\n",
    "\n",
    "input_code = sagemaker_session.upload_data(\n",
    "    PREPROCESSING_SCRIPT_LOCATION,\n",
    "    bucket = sagemaker_session.default_bucket(),\n",
    "    key_prefix = \"processing/code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 locations of preprocessing output and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ScriptProcessor` class lets you run a command inside the container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "preprocessing_processor = ScriptProcessor(\n",
    "    command = ['python3'],\n",
    "    image_uri = processing_repository_uri,\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
